<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
      
    
    
    
    
    
      
    
    
    
    
      
    
    
      
    
    
      
    
    <title>Local LLMs, METAL Crashes, and the Cloud Fallback — kanthi</title>
    <meta name="description" content="Tried running LLMs locally with MLX on Apple Silicon. Learned that local inference is promising but fragile. Ended up using OpenRouter as a fallback.">
    <meta name="robots" content="index,follow">
    
      <meta name="author" content="Akshay Kanthi">
    
    <link rel="canonical" href="https://kanthi.io/local-llms-metal-crashes/">
    <meta property="og:site_name" content="kanthi">
    <meta property="og:title" content="Local LLMs, METAL Crashes, and the Cloud Fallback — kanthi">
    <meta property="og:description" content="Tried running LLMs locally with MLX on Apple Silicon. Learned that local inference is promising but fragile. Ended up using OpenRouter as a fallback.">
    <meta property="og:url" content="https://kanthi.io/local-llms-metal-crashes/">
    <meta property="og:type" content="article">
    
      <meta property="og:image" content="https://kanthi.io/assets/IMG_9015-01.jpeg">
      <meta property="og:image:alt" content="Local LLMs, METAL Crashes, and the Cloud Fallback">
      <meta property="og:image:width" content="1200">
      <meta property="og:image:height" content="630">
    
    <meta name="twitter:card" content="summary_large_image">
    
      <meta name="twitter:creator" content="@aksanoble">
    
    <meta name="twitter:title" content="Local LLMs, METAL Crashes, and the Cloud Fallback — kanthi">
    <meta name="twitter:description" content="Tried running LLMs locally with MLX on Apple Silicon. Learned that local inference is promising but fragile. Ended up using OpenRouter as a fallback.">
    
      <meta name="twitter:image" content="https://kanthi.io/assets/IMG_9015-01.jpeg">
    
    
      <meta property="article:published_time" content="2026-01-31T00:00:00.000Z">
      <meta property="article:modified_time" content="2026-01-31T00:00:00.000Z">
    
    
      <link rel="alternate" type="application/rss+xml" title="kanthi RSS" href="https://kanthi.io/feed.xml">
    
    <script type="application/ld+json">
      {"@context":"https://schema.org","@type":"WebSite","name":"kanthi","url":"https://kanthi.io","description":"Personal website and digital garden of Akshay Kanthi","about":{"@type":"Person","name":"Akshay Kanthi","url":"https://kanthi.io/about/","sameAs":["https://x.com/aksanoble","https://linkedin.com/in/akshaykanthi"]},"publisher":{"@type":"Person","name":"Akshay Kanthi","url":"https://kanthi.io/about/","sameAs":["https://x.com/aksanoble","https://linkedin.com/in/akshaykanthi"]}}
    </script>
    
      <script type="application/ld+json">
        {"@context":"https://schema.org","@type":"BlogPosting","headline":"Local LLMs, METAL Crashes, and the Cloud Fallback","description":"Tried running LLMs locally with MLX on Apple Silicon. Learned that local inference is promising but fragile. Ended up using OpenRouter as a fallback.","url":"https://kanthi.io/local-llms-metal-crashes/","datePublished":"2026-01-31T00:00:00.000Z","dateModified":"2026-01-31T00:00:00.000Z","image":"https://kanthi.io/assets/IMG_9015-01.jpeg","author":{"@type":"Person","name":"Akshay Kanthi","url":"https://kanthi.io/about/","sameAs":["https://x.com/aksanoble","https://linkedin.com/in/akshaykanthi"]}}
      </script>
    
    <link rel="icon" href="/favicon.svg" type="image/svg+xml">
    <link rel="stylesheet" href="/assets/styles.css">
    
      <link rel="preconnect" href="https://fonts.googleapis.com">
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link href="https://fonts.googleapis.com/css2?family=Dawning+of+a+New+Day&family=DM+Sans:wght@400;500;700&family=Playfair+Display:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
    
  </head>
  <body class="page-post">
    
    
      <div class="site-frame">
    
      <header>
  <div class="brand">
    <a class="brand-link" href="/">KANTHI</a>
  </div>
  <nav>
    
      
      <a href="/archive/">ARCHIVE</a>
    
      
      <a href="/about/">ABOUT</a>
    
    <a href="/feed.xml">RSS</a>
  </nav>
</header>

      <main class="article-main">
  <article class="article-block">
    <div class="article-meta">
      <span class="article-index">Jan 31</span>
      <span>LOG</span>
    </div>

    <h1 class="type-display article-title">Local LLMs, METAL Crashes, and the Cloud Fallback</h1>

    
      <p class="article-dek">Tried running LLMs locally with MLX on Apple Silicon. Learned that local inference is promising but fragile. Ended up using OpenRouter as a fallback.</p>
    

    

    <div class="article-body">
      <p>The entity extraction in entt-agent was working, but I wanted better. Enter <a href="https://github.com/HKUDS/LightRAG">LightRAG</a> - a library that does graph-based RAG with automatic entity and relationship discovery. Exactly what I needed.</p>
<p>One problem: it needs an LLM to work.</p>
<p>The romantic in me wanted everything local. My data, my models, my hardware. So I went down the MLX rabbit hole.</p>
<p><a href="https://github.com/ml-explore/mlx">MLX</a> is Apple's framework for running ML models on Apple Silicon. I installed <code>mlx-lm</code>, downloaded <strong>Qwen2.5-Coder-14B-Instruct-4bit</strong> (9GB of quantized weights), and prepared for local AI magic.</p>
<p>The first few prompts worked beautifully. Then:</p>
<pre><code>libc++abi: terminating due to uncaught exception of type
std::runtime_error: [METAL] Command buffer execution failed
</code></pre>
<p>METAL buffer crashes. The GPU ran out of memory, or something. I tried smaller context windows. I tried different models. I tried DeepSeek R1 distilled variants. Same story - works until it doesn't.</p>
<p>The pragmatist in me gave up and set up <a href="https://openrouter.ai/">OpenRouter</a>. DeepSeek-R1T2-Chimera via API. Reliable, fast, not running on my laptop.</p>
<p>The lesson: local LLMs on Apple Silicon are <em>almost</em> there. The sweet spot might be local embeddings (small, fast, no API calls) plus cloud LLM for generation (reliable, scalable). Hybrid approach. Not as pure, but it actually works.</p>
<p>Sometimes &quot;actually works&quot; beats &quot;philosophically correct.&quot;</p>

    </div>
  </article>
</main>

      <footer class="site-footer">
  <div class="footer-content">
    <div class="footer-links">
      
        
          <a href="https://x.com/aksanoble">X</a>
        
          <a href="https://linkedin.com/in/akshaykanthi">LinkedIn</a>
        
      
      <a href="/feed.xml">RSS</a>
    </div>
  </div>
</footer>

    
      </div>
    
  </body>
</html>
